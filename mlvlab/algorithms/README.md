# **Arquitectura Sistema Algoritmos MLV-Lab**

La filosof√≠a de este sistema es la **separaci√≥n de responsabilidades**. Cada componente tiene un trabajo muy espec√≠fico, lo que hace que el framework sea flexible, robusto y, lo m√°s importante, f√°cil de extender.

## **1\. El Lanzador (The Launcher): mlvlab/cli/main.py üöÄ**

Este es el **punto de entrada principal** que ve el usuario en la terminal.

* **Responsabilidad:** Su √∫nico trabajo es interpretar los comandos (play, train, eval) y los argumentos que le pasas (como el env\_id).  
* **NO contiene l√≥gica de juego ni de IA.** En lugar de hacer el trabajo √©l mismo, **lanza un nuevo proceso de Python** ejecutando un script "ayudante" (como train\_entry.py o player\_entry.py).  
* **¬øPor qu√©?** Esto mantiene la interfaz de l√≠nea de comandos (CLI) limpia y r√°pida, y a√≠sla cada tarea en su propio proceso, evitando conflictos de librer√≠as o estados.

## **2\. Los Puntos de Entrada (The Entry Points): mlvlab/cli/\*\_entry.py üö™**

Estos son los **guiones intermediarios** que el lanzador ejecuta. Son el puente entre la petici√≥n del usuario y la l√≥gica real del algoritmo.

* **Responsabilidad:** Toman los argumentos de la l√≠nea de comandos, cargan la configuraci√≥n inicial del entorno (get\_env\_config), y preparan todo lo necesario para llamar al algoritmo correcto.  
* train\_entry.py, por ejemplo, carga el config.py del entorno para saber qu√© algoritmo usar (ALGORITHM \= "ql" o "random"), prepara el directorio para guardar los resultados (run\_dir), y finalmente llama al m√©todo train del plugin correspondiente.

## **3\. El Coraz√≥n del Sistema: El Framework de Plugins (mlvlab/algorithms/) ‚ù§Ô∏è**

Esta es la parte m√°s importante y la que hace que el sistema sea extensible.

#### **El Contrato (registry.py)**

El archivo registry.py define el protocolo AlgorithmPlugin. Esto es como un **contrato** que obliga a que todos los algoritmos tengan exactamente los mismos m√©todos (key, build\_agent, train, eval). Esto garantiza que train\_entry.py pueda llamar a algo.train(...) sin importar si algo es Q-Learning, un Agente Aleatorio o DQN.

Tambi√©n contiene el **registro central \_ALGORITHMS**, que es como una agenda telef√≥nica. Cada plugin, al ser importado, se apunta en esta agenda usando su key.

#### **La Detecci√≥n Autom√°tica (\_\_init\_\_.py)**

Este archivo es el "mago" del sistema. En lugar de tener que importar manualmente cada plugin nuevo que crees, este script **escanea autom√°ticamente** todas las subcarpetas dentro de mlvlab/algorithms/.

Si encuentra una carpeta que contiene un archivo plugin.py, lo importa. El propio plugin.py se auto-registra al ser importado (gracias a la l√≠nea register\_algorithm(...) al final del archivo).

**Resultado:** Para a√±adir un nuevo algoritmo, solo tienes que crear su carpeta y su plugin.py. ¬°No necesitas tocar ning√∫n otro archivo de la estructura central\!

#### **Los Plugins (ql/plugin.py, random/plugin.py, etc.)**

Estos son los **trabajadores reales**. Cada plugin es una caja negra autosuficiente que contiene toda la l√≥gica espec√≠fica de su algoritmo.

* El QLearningPlugin, por ejemplo, sabe que necesita un state\_adapter, sabe c√≥mo construir un QLearningAgent, sabe c√≥mo ejecutar el bucle de entrenamiento learn, y sabe c√≥mo guardar la q\_table.  
* train\_entry.py no sabe nada de esto, simplemente le dice: "Oye, Q-Learning, entr√©nate con esta configuraci√≥n".

## **4\. Relaci√≥n entre AlgorithmPlugin y Agent üßë‚Äçüè´üß†**

Esta es la distinci√≥n m√°s importante de la arquitectura. En resumen: el **AlgorithmPlugin** es el **entrenador**, mientras que el **Agent** es el **aprendiz** o el **cerebro**.

#### **Una Analog√≠a: El Entrenador y el Atleta**

* El **Agent** (ej: QLearningAgent) es como un **atleta**. Tiene el conocimiento espec√≠fico de su disciplina: sabe c√≥mo ejecutar una acci√≥n (act) y c√≥mo mejorar su t√©cnica a partir de los resultados (learn). Su estado interno (su "memoria muscular" o conocimiento) es la Q-Table o los pesos de una red neuronal.  
* El **AlgorithmPlugin** (ej: QLearningPlugin) es el **entrenador**. No corre la carrera, pero dise√±a y dirige todo el plan de entrenamiento (train).  
  * **build\_agent**: El entrenador "ficha" o crea a su atleta.  
  * **train**: El entrenador dise√±a la rutina: pone al atleta en la pista (el env), le dice cu√°ndo correr, observa los resultados y le dice al atleta que aprenda de ellos (agent.learn).  
  * **eval**: El entrenador pone a prueba al atleta en una competici√≥n para ver su rendimiento.

#### **Responsabilidades Detalladas**

* **AlgorithmPlugin (El Orquestador)**  
  * **Gestiona el flujo:** Controla el bucle principal de episodios y pasos (for episode in episodes...).  
  * **Interact√∫a con el exterior:** Es el √∫nico que habla con el env (haciendo env.step()) y con el sistema de archivos (guardando y cargando en el run\_dir).  
  * **Crea el agente:** Usa build\_agent para instanciar el agente con los hiperpar√°metros correctos que vienen del archivo config.py.  
  * **Es gen√©rico:** Su estructura es la misma para cualquier algoritmo (siempre tiene train y eval).  
* **Agent (El Cerebro)**  
  * **Contiene la l√≥gica de RL:** Implementa la f√≥rmula matem√°tica del algoritmo (la actualizaci√≥n de la Q-Table, el forward pass de la red neuronal, etc.).  
  * **Toma decisiones:** Su m√©todo act o predict decide qu√© acci√≥n tomar dada una observaci√≥n.  
  * **Aprende:** Su m√©todo learn actualiza su estado interno (la Q-Table, los pesos de la red) bas√°ndose en la experiencia (obs, action, reward, next\_obs).  
  * **Es espec√≠fico:** Un QLearningAgent es muy diferente de un DQNAgent.

## **5\. Gu√≠a Pr√°ctica: C√≥mo A√±adir un Nuevo Algoritmo (DQN)**

Siguiendo la l√≥gica anterior, a√±adir un agente DQN ser√≠a un proceso muy sistem√°tico. La clave es replicar el patr√≥n de dise√±o de tu QLearningPlugin, que delega el trabajo pesado a funciones "helper".

### **Comparaci√≥n de Patrones: RandomPlugin vs. QLearningPlugin**

* **RandomPlugin**: Es simple. El bucle de entrenamiento y evaluaci√≥n est√° escrito **directamente dentro** de los m√©todos train y eval del plugin. Esto est√° bien para algoritmos sencillos.  
* **QLearningPlugin**: Es m√°s avanzado y robusto. En lugar de tener el bucle dentro, **llama a funciones externas** (train\_with\_state\_adapter, evaluate\_with\_optional\_recording) que contienen la l√≥gica reutilizable. **Este es el patr√≥n que debemos seguir para DQN.**

### **Paso 1: Crear la L√≥gica del Agente**

Primero, necesitas el c√≥digo del agente en s√≠. Este archivo contiene la clase con la red neuronal, el replay buffer, y los m√©todos act, learn, save y load.

**Nuevo Archivo: mlvlab/agents/dqn\_agent.py**

```python
# mlvlab/agents/dqn_agent.py
import torch
# ... importaciones de PyTorch ...

class DQNAgent:
    def __init__(self, observation_space, action_space, ...):
        # L√≥gica de la red neuronal, optimizador, replay buffer, etc.
        pass

    def act(self, obs, epsilon=0.0):
        # L√≥gica para elegir una acci√≥n (con epsilon-greedy)
        pass

    def learn(self):
        # L√≥gica para entrenar la red con un lote del replay buffer
        pass
    
    def save(self, filepath):
        # Guardar los pesos del modelo (model.state_dict())
        pass

    def load(self, filepath):
        # Cargar los pesos del modelo
        pass
```

### **Paso 2: Crear la Carpeta del Plugin**

Dentro de mlvlab/algorithms/, crea una nueva carpeta para tu algoritmo.

* mlvlab/algorithms/dqn/

### **Paso 3: Implementar el Plugin (Patr√≥n Avanzado)**

Dentro de la nueva carpeta, crea el archivo plugin.py. Este plugin no contendr√° el bucle de entrenamiento, sino que llamar√° a una funci√≥n "helper" que s√≠ lo har√° (igual que hace el QLearningPlugin).

**Nuevo Archivo: mlvlab/algorithms/dqn/plugin.py**

```python
from __future__ import annotations
from pathlib import Path
from typing import Any, Dict, Optional
import gymnasium as gym

from mlvlab.algorithms.registry import register_algorithm
from mlvlab.agents.dqn_agent import DQNAgent # <-- Importas tu nuevo agente
# from mlvlab.helpers.train import train_with_replay_buffer # <-- Importar√≠as un nuevo helper
# from mlvlab.helpers.eval import evaluate_with_optional_recording # <-- Reutilizas el helper de eval

class DQNPlugin:
    def key(self) -> str:
        return "dqn"

    def build_agent(self, env: gym.Env, hparams: Dict[str, Any]) -> DQNAgent:
        # Crea una instancia de tu agente DQN con los hiperpar√°metros
        return DQNAgent(
            env.observation_space,
            env.action_space,
            learning_rate=hparams.get("alpha", 0.001),
            # ... otros hiperpar√°metros del config.py ...
        )

    def train(self, env_id: str, config: Dict[str, Any], run_dir: Path, seed: Optional[int] = None, render: bool = False) -> None:
        # El plugin prepara el 'agent_builder' y delega el bucle a un helper
        def agent_builder(env: gym.Env) -> DQNAgent:
            return self.build_agent(env, config)

        # Aqu√≠ llamar√≠as a una funci√≥n helper que contenga el bucle de entrenamiento de DQN
        # train_with_replay_buffer(
        #     env_id=env_id,
        #     run_dir=run_dir,
        #     total_episodes=int(config.get("episodes", 1000)),
        #     agent_builder=agent_builder,
        #     seed=seed,
        #     render=render,
        #     # ... otros par√°metros espec√≠ficos de DQN (tama√±o del buffer, etc.) ...
        # )
        print("L√≥gica de entrenamiento para DQN ir√≠a aqu√≠, llamando a un helper.")


    def eval(self, env_id: str, run_dir: Path, **kwargs: Any) -> Optional[str]:
        # La evaluaci√≥n es similar: preparas el builder y llamas al helper
        def builder(env: gym.Env) -> DQNAgent:
            agent = self.build_agent(env, {})
            # El agente DQN sabe c√≥mo cargar sus propios pesos
            agent.load(run_dir / "model.pth") 
            return agent

        # Reutilizas el mismo helper de evaluaci√≥n que Q-Learning
        # evaluate_with_optional_recording(
        #     env_id=env_id,
        #     run_dir=run_dir,
        #     agent_builder=builder,
        #     **kwargs
        # )
        print("L√≥gica de evaluaci√≥n para DQN ir√≠a aqu√≠, llamando a un helper.")
        return None

# ¬°El paso m√°gico! El plugin se registra a s√≠ mismo.
register_algorithm(DQNPlugin())
```

### **Paso 4: Actualizar la Configuraci√≥n de un Entorno**

Para usar tu nuevo agente, solo tendr√≠as que ir al config.py de cualquier entorno y cambiar el algoritmo.

**Archivo: mlvlab/envs/un\_entorno\_cualquiera/config.py**

```python
# ...
ALGORITHM = "dqn" # <-- ¬°Listo!

BASELINE = {
    "config": {
        "episodes": 5000,
        "alpha": 0.001, # Learning rate para DQN
        # ... otros hiperpar√°metros para DQN ...
    }
}
# ...
```

¬°Y ya est√°\! Al ejecutar mlvlab train un-entorno-cualquiera, el sistema detectar√≠a tu nuevo plugin dqn y ejecutar√≠a la l√≥gica que acabas de definir.